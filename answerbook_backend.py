# -*- coding: utf-8 -*-
"""bookbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x-bf9-oCmSRAzDuzLepZqvUHa2KK8ndk
"""

import os

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "Fill"

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install chromadb==0.4.10 tiktoken==0.3.3 sqlalchemy==2.0.15
# !pip install langchain==0.0.249
# !pip install --force-reinstall pydantic==1.10.6
# !pip install sentence_transformers

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, ConversationalRetrievalChain, ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import messages_from_dict, messages_to_dict
from langchain.memory.chat_message_histories.in_memory import ChatMessageHistory
from langchain.agents import Tool
from langchain.agents import initialize_agent
from langchain.agents import AgentType

cache_dir = "/content/"

import pandas as pd
pd.set_option('display.max_column', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_seq_items', None)
pd.set_option('display.max_colwidth', 500)
pd.set_option('expand_frame_repr', True)

from langchain.document_loaders import GutenbergLoader

loader = GutenbergLoader(
    "https://www.gutenberg.org/cache/epub/1268/pg1268.txt"
)

document = loader.load()

extrait = ' '.join(document[0].page_content.split()[:100])
display(extrait + " .......")

from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
import tempfile

# Chunk sizes of 1024 and an overlap of 256 (this will take approx. 10mins with this model to build our vector database index)
text_splitter = CharacterTextSplitter(
    chunk_size=1024,
    chunk_overlap=256
)
texts = text_splitter.split_documents(document)

model_name = "sentence-transformers/all-MiniLM-L6-v2"

embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    cache_folder=cache_dir
)  # Use a pre-cached model

vectordb = Chroma.from_documents(
    texts,
    embeddings,
    persist_directory=cache_dir
)

from langchain.llms import HuggingFacePipeline

# We want to make this a retriever, so we need to convert our index.
# This will create a wrapper around the functionality of our vector database
# so we can search for similar documents/chunks in the vectorstore and retrieve the results:
retriever = vectordb.as_retriever()

# This chain will be used to do QA on the document. We will need
# 1 - A LLM to do the language interpretation
# 2 - A vector database that can perform document retrieval
# 3 - Specification on how to deal with this data

hf_llm = HuggingFacePipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    model_kwargs={
#        "temperature": 0,
        "do_sample":True,
        "max_length": 2048,
        "cache_dir": cache_dir,
    },
)

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=hf_llm,
    chain_type="refine",
    retriever=retriever
)
query = "Who is the main character?"
query_results_venice = qa.run(query)
print("#" * 12)
query_results_venice

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=hf_llm,
    chain_type="refine",
    retriever=retriever
)
query = "What is the name of the island?"
query_results_venice = qa.run(query)
print("#" * 12)
query_results_venice

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=hf_llm,
    chain_type="refine",
    retriever=retriever
)
query = "When was the book 'The Mysterious Island' written?"
query_results_venice = qa.run(query)
print("#" * 12)
query_results_venice

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=hf_llm,
    chain_type="refine",
    retriever=retriever
)
query = "Who was the book 'The Mysterious Island' written?"
query_results_venice = qa.run(query)
print("#" * 12)
query_results_venice

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=hf_llm,
    chain_type="refine",
    retriever=retriever
)
query = "what is the Nautilus?"
query_results_venice = qa.run(query)
print("#" * 12)
query_results_venice

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=hf_llm,
    chain_type="refine",
    retriever=retriever
)
query = "what happened on 22nd of June, 1867?"
query_results_venice = qa.run(query)
print("#" * 12)
query_results_venice

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=hf_llm,
    chain_type="refine",
    retriever=retriever
)
query = "How do they know the Captain Nemo?"
query_results_venice = qa.run(query)
print("#" * 12)
query_results_venice