# -*- coding: utf-8 -*-
"""bookbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x-bf9-oCmSRAzDuzLepZqvUHa2KK8ndk
"""

import os

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "Fill"

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install chromadb==0.4.10 tiktoken==0.3.3 sqlalchemy==2.0.15
# !pip install langchain==0.0.249
# !pip install --force-reinstall pydantic==1.10.6
# !pip install sentence_transformers

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, ConversationalRetrievalChain, ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import messages_from_dict, messages_to_dict
from langchain.memory.chat_message_histories.in_memory import ChatMessageHistory
from langchain.agents import Tool
from langchain.agents import initialize_agent
from langchain.agents import AgentType

cache_dir = "/content/"

import pandas as pd
pd.set_option('display.max_column', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.max_seq_items', None)
pd.set_option('display.max_colwidth', 500)
pd.set_option('expand_frame_repr', True)

from langchain.document_loaders import GutenbergLoader

loader = GutenbergLoader(
    "https://www.gutenberg.org/cache/epub/1268/pg1268.txt"
)

document = loader.load()

extrait = ' '.join(document[0].page_content.split()[:100])
display(extrait + " .......")

from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
import tempfile

# Chunk sizes of 1024 and an overlap of 256 (this will take approx. 10mins with this model to build our vector database index)
text_splitter = CharacterTextSplitter(
    chunk_size=1024,
    chunk_overlap=256
)
texts = text_splitter.split_documents(document)

model_name = "sentence-transformers/all-MiniLM-L6-v2"

embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    cache_folder=cache_dir
)  # Use a pre-cached model

vectordb = Chroma.from_documents(
    texts,
    embeddings,
    persist_directory=cache_dir
)

from langchain.llms import HuggingFacePipeline

# We want to make this a retriever, so we need to convert our index.
# This will create a wrapper around the functionality of our vector database
# so we can search for similar documents/chunks in the vectorstore and retrieve the results:
retriever = vectordb.as_retriever()

# This chain will be used to do QA on the document. We will need
# 1 - A LLM to do the language interpretation
# 2 - A vector database that can perform document retrieval
# 3 - Specification on how to deal with this data

hf_llm = HuggingFacePipeline.from_model_id(
    model_id="google/flan-t5-large",
    task="text2text-generation",
    model_kwargs={
#        "temperature": 0,
        "do_sample":True,
        "max_length": 2048,
        "cache_dir": cache_dir,
    },
)

#!pip install ipywidgets

from langchain.chains import RetrievalQA
from ipywidgets import widgets, Layout, VBox
from IPython.display import display

# Función para manejar la consulta del usuario y ejecutarla
def ejecutar_consulta(b):
    query = input_text.value
    if query.lower() == "salir":
        output_text.value = "Saliendo del programa..."
    else:
        qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type="refine", retriever=retriever)
        query_results = qa.run(query)
        output_text.value = str(query_results)

# Crear widgets de entrada y salida
input_text = widgets.Text(placeholder='Ingrese su pregunta...', layout=Layout(width='70%'))
output_text = widgets.Textarea(layout=Layout(width='70%', height='200px'))

# Crear botón para enviar la pregunta
submit_button = widgets.Button(description='Enviar', button_style='success', layout=Layout(width='20%'))
submit_button.on_click(ejecutar_consulta)

# Colocar widgets en una caja vertical
chat_box = VBox([input_text, submit_button, output_text])

# Mostrar el chatbox
display(chat_box)
